apiVersion: storage.gcp.upbound.io/v1beta1
kind: BucketObject
metadata:
  annotations:
    # Note from the upbound marketplace example:
    upjet.upbound.io/manual-intervention: BucketObject does not support source field
    # This would be great to have ... otherwise we're down to text files
  labels:
    testing.upbound.io/example-name: input
  name: input
spec:
  forProvider:
    bucketSelector:
      matchLabels:
        testing.upbound.io/example-name: crossplane-bucket
    #########################################################################################
    # BucketObject does not support source field 
    # https://github.com/upbound/provider-gcp/issues/9
    #########################################################################################
    # contentType: application/zip
    # name: test.zip
    # source: /home/martinfleming/src/crossplane/gcp/crossplane-gcp/resources/Storage/test.zip
    # 
    # when using "source" field, getting this error even though file exists
    # apply failed: open /home/martinfleming/src/crossplane/gcp/crossplane-gcp/resources/Storage/test.zip: 
    # no such file or directory:
    #########################################################################################
    content: "Content of text file goes here"
    contentType: text/plain
    name: input.txt
---
apiVersion: storage.gcp.upbound.io/v1beta1
kind: BucketObject
metadata:
  labels:
    testing.upbound.io/example-name: output
  name: output
spec:
  forProvider:
    bucketSelector:
      matchLabels:
        testing.upbound.io/example-name: crossplane-bucket
    content: "Content of text file goes here"
    contentType: text/plain
    name: output.txt
---
apiVersion: storage.gcp.upbound.io/v1beta1
kind: BucketObject
metadata:
  labels:
    testing.upbound.io/example-name: temp
  name: temp
spec:
  forProvider:
    bucketSelector:
      matchLabels:
        testing.upbound.io/example-name: crossplane-bucket
    # contentType: Folder   # not needed, but can be included
    name: temp/             # folder name should end with '/'
    content: ' '            # content is ignored but should be non-empty
---
apiVersion: storage.gcp.upbound.io/v1beta1
kind: BucketObject
metadata:
  labels:
    testing.upbound.io/example-name: staging
  name: staging
spec:
  forProvider:
    bucketSelector:
      matchLabels:
        testing.upbound.io/example-name: crossplane-bucket

    name: staging/          # folder name should end with '/'
    content: ' '            # content is ignored but should be non-empty
---
apiVersion: storage.gcp.upbound.io/v1beta1
kind: BucketObject
metadata:
  labels:
    testing.upbound.io/example-name: wordcount
  name: wordcount
spec:
  forProvider:
    bucketSelector:
      matchLabels:
        testing.upbound.io/example-name: crossplane-bucket
    content: |
      import argparse
      import re

      import apache_beam as beam
      from apache_beam.options.pipeline_options import PipelineOptions
      from apache_beam.options.pipeline_options import GoogleCloudOptions
      from apache_beam.options.pipeline_options import SetupOptions
      from apache_beam.options.pipeline_options import StandardOptions

      class WordCount(beam.PTransform):
          def expand(self, pcoll):
              return (
                  pcoll
                  | 'ExtractWords' >> beam.FlatMap(lambda x: re.findall(r'[A-Za-z\']+', x))
                  | 'CountWords' >> beam.combiners.Count.PerElement()
              )

      def run(argv=None, save_main_session=True):
          parser = argparse.ArgumentParser()
          parser.add_argument('--inputFile', required=True,
                              help='Input file to process.')
          parser.add_argument('--outputFile', required=True,
                              help='Output file to write results to.')
          known_args, pipeline_args = parser.parse_known_args(argv)

          pipeline_options = PipelineOptions(pipeline_args)
          google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)
          google_cloud_options.project = 'YOUR_PROJECT_ID'
          google_cloud_options.job_name = 'your-job-name'
          google_cloud_options.staging_location = 'gs://your-bucket/staging'
          google_cloud_options.temp_location = 'gs://your-bucket/temp'
          pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'
          pipeline_options.view_as(SetupOptions).save_main_session = save_main_session

          with beam.Pipeline(options=pipeline_options) as p:
              lines = p | 'ReadFromText' >> beam.io.ReadFromText(known_args.inputFile)
              counts = lines | WordCount()
              counts | 'WriteToText' >> beam.io.WriteToText(known_args.outputFile, shard_name_template='')

      if __name__ == '__main__':
          run()
    contentType: text/plain
    name: word_count_template.py